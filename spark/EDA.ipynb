{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c103e12",
   "metadata": {},
   "source": [
    "# 0. Libraries and spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95a9b339",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spark-nlp==3.3.4 in /usr/local/lib/python3.6/dist-packages\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install spark-nlp==3.3.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c32acb2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.6/dist-packages (from nltk)\n",
      "Requirement already satisfied: click in /usr/lib/python3/dist-packages (from nltk)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from nltk)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from nltk)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.7.3->pandas)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install numpy pandas nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da483f4c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://ip-172-31-7-229.ec2.internal:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fdded194c18>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b5ac11a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sparknlp.annotator import LemmatizerModel\n",
    "import pyspark.sql.functions as f\n",
    "from pyspark.sql import Window\n",
    "import pyspark.sql.types as t\n",
    "from pyspark.ml.feature import Tokenizer as pysparkTokenizer, HashingTF, StopWordsRemover, CountVectorizer\n",
    "from pyspark.ml import Pipeline\n",
    "from sparknlp.base import DocumentAssembler, Finisher\n",
    "from sparknlp.annotator import Tokenizer, Normalizer, LemmatizerModel, StopWordsCleaner, PerceptronModel, Chunker\n",
    "from pyspark.ml.clustering import LDA\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c7fe3c",
   "metadata": {},
   "source": [
    "# 1. Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d109779f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|                text|\n",
      "+--------------------+\n",
      "|RT @votetimscott:...|\n",
      "|RT @HouseGOP: htt...|\n",
      "|RT @HouseGOP: Thi...|\n",
      "|RT @POTUS: When I...|\n",
      "|RT @POTUS: Here’s...|\n",
      "|RT @POTUS: My Adm...|\n",
      "|@JoeBiden and wea...|\n",
      "|@RandPaul https:/...|\n",
      "|@RandPaul Also, m...|\n",
      "|RT @SenWarren: Th...|\n",
      "|@amyklobuchar SER...|\n",
      "|@VP I am old enou...|\n",
      "|RT @VP: There is ...|\n",
      "|RT @POTUS: Folks,...|\n",
      "|RT @JoeBiden: As ...|\n",
      "|RT @JoeBiden: The...|\n",
      "|@POTUS Just becau...|\n",
      "|RT @POTUS: Here’s...|\n",
      "|RT @SenSchumer: W...|\n",
      "|RT @POTUS: My Adm...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Lenght: 7379'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.read.option(\"tableName\", \"Tweets\").format(\"dynamodb\").load().select(f.col(\"text\"))\\\n",
    ".union(\n",
    "    spark.read.option(\"tableName\", \"RedditPosts\").format(\"dynamodb\").load().select(f.col(\"submission_id\").alias(\"text\"))\n",
    ")\n",
    "df.show()\n",
    "f\"Lenght: {df.count()}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5308ca20",
   "metadata": {},
   "source": [
    "Removing special chacaters and changing to lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7c05c398",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|                text|\n",
      "+--------------------+\n",
      "|rt @votetimscott ...|\n",
      "|rt @housegop http...|\n",
      "|rt @housegop this...|\n",
      "|rt @potus when i ...|\n",
      "|rt @potus heres w...|\n",
      "|rt @potus my admi...|\n",
      "|@joebiden and wea...|\n",
      "|@randpaul httpstc...|\n",
      "|@randpaul also ma...|\n",
      "|rt @senwarren the...|\n",
      "|@amyklobuchar ser...|\n",
      "|@vp i am old enou...|\n",
      "|rt @vp there is a...|\n",
      "|rt @potus folks i...|\n",
      "|rt @joebiden as w...|\n",
      "|rt @joebiden the ...|\n",
      "|@potus just becau...|\n",
      "|rt @potus heres w...|\n",
      "|rt @senschumer wi...|\n",
      "|rt @potus my admi...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.withColumn(\"text\", f.lower(f.regexp_replace(f.col(\"text\"), \"[^A-Za-z0-9@ ]\", \"\")))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13867c2a",
   "metadata": {},
   "source": [
    "# 2. LDA - topics analysis - Pyspark only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "76ece113",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+\n",
      "|                text|              tokens|       no stop words|\n",
      "+--------------------+--------------------+--------------------+\n",
      "|rt @votetimscott ...|[rt, @votetimscot...|[rt, @votetimscot...|\n",
      "|rt @housegop http...|[rt, @housegop, h...|[rt, @housegop, h...|\n",
      "|rt @housegop this...|[rt, @housegop, t...|[rt, @housegop, j...|\n",
      "|rt @potus when i ...|[rt, @potus, when...|[rt, @potus, took...|\n",
      "|rt @potus heres w...|[rt, @potus, here...|[rt, @potus, here...|\n",
      "+--------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'text': 'rt @votetimscott one of the highlights of my year was giving the official @gop response to joe bidens first address to congress i said i',\n",
       "  'tokens': ['rt',\n",
       "   '@votetimscott',\n",
       "   'one',\n",
       "   'of',\n",
       "   'the',\n",
       "   'highlights',\n",
       "   'of',\n",
       "   'my',\n",
       "   'year',\n",
       "   'was',\n",
       "   'giving',\n",
       "   'the',\n",
       "   'official',\n",
       "   '@gop',\n",
       "   'response',\n",
       "   'to',\n",
       "   'joe',\n",
       "   'bidens',\n",
       "   'first',\n",
       "   'address',\n",
       "   'to',\n",
       "   'congress',\n",
       "   'i',\n",
       "   'said',\n",
       "   'i'],\n",
       "  'no stop words': ['rt',\n",
       "   '@votetimscott',\n",
       "   'one',\n",
       "   'highlights',\n",
       "   'year',\n",
       "   'giving',\n",
       "   'official',\n",
       "   '@gop',\n",
       "   'response',\n",
       "   'joe',\n",
       "   'bidens',\n",
       "   'first',\n",
       "   'address',\n",
       "   'congress',\n",
       "   'said']},\n",
       " {'text': 'rt @housegop httpstcosaozqqvrmn',\n",
       "  'tokens': ['rt', '@housegop', 'httpstcosaozqqvrmn'],\n",
       "  'no stop words': ['rt', '@housegop', 'httpstcosaozqqvrmn']},\n",
       " {'text': 'rt @housegop this is joe bidens america httpstco9yhn34psbr',\n",
       "  'tokens': ['rt',\n",
       "   '@housegop',\n",
       "   'this',\n",
       "   'is',\n",
       "   'joe',\n",
       "   'bidens',\n",
       "   'america',\n",
       "   'httpstco9yhn34psbr'],\n",
       "  'no stop words': ['rt',\n",
       "   '@housegop',\n",
       "   'joe',\n",
       "   'bidens',\n",
       "   'america',\n",
       "   'httpstco9yhn34psbr']}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = pysparkTokenizer(inputCol=\"text\", outputCol=\"tokens\")\n",
    "stopwords_cleaner = StopWordsRemover(inputCol=\"tokens\", outputCol=\"no stop words\")\n",
    "nlp_pipeline = Pipeline(\n",
    "    stages=[tokenizer,\n",
    "            stopwords_cleaner])\n",
    "nlp_model = nlp_pipeline.fit(df)\n",
    "processed_df  = nlp_model.transform(df)\n",
    "processed_df.show(5)\n",
    "processed_df.limit(3).toPandas().to_dict(\"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f0f0009a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', 'new', '@potus', 'one', 'jobs', 'president', '6', 'million', 'year', 'rt', 'record', 'stand', 'unemployment', 'heres', 'nearly'] \n",
      "\n",
      "['rt', '@potus', 'covid19', '', '@randpaul', 'high', 'booster', 'increase', 'shots', 'protection', 'antibody', 'degree', 'severe', 'providing', 'levels'] \n",
      "\n",
      "['@potus', '@vp', 'rt', 'get', 'people', '@randpaul', 'covid', 'dont', 'biden', 'joe', 'voting', 'amp', 'know', 'us', '@joebiden'] \n",
      "\n",
      "['rt', '@senwarren', 'care', 'americans', 'want', 'law', 'court', '@randpaul', '70', 'supreme', 'ready', 'radical', 'land', 'roevwade', 'ove'] \n",
      "\n",
      "['@joebiden', 'rt', 'way', 'best', 'continue', 'protect', 'year', 'remember', 'health', 'head', 'holiday', 'new', 'celebrations', 'kee', '@tedcruz'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "cv = CountVectorizer(inputCol=\"no stop words\", outputCol=\"features\", vocabSize=500, minDF=3.0)\n",
    "cv_model = cv.fit(processed_df)\n",
    "lda = LDA(k=5, maxIter=100)\n",
    "model = lda.fit(cv_model.transform(processed_df))\n",
    "for indices in model.describeTopics(15).select(\"termIndices\").rdd.flatMap(lambda x: x).collect():\n",
    "    print([cv_model.vocabulary[i] for i in indices], '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00101ca",
   "metadata": {},
   "source": [
    "# 3. LDA - topics analysis - SparkNLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "00280e7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lemma_antbnc download started this may take some time.\n",
      "Approximate size to download 907.6 KB\n",
      "[OK!]\n",
      "pos_anc download started this may take some time.\n",
      "Approximate size to download 3.9 MB\n",
      "[OK!]\n",
      "+--------------------+--------------------+--------------------+--------------------+\n",
      "|                text|   finished_unigrams|     finished_ngrams|               final|\n",
      "+--------------------+--------------------+--------------------+--------------------+\n",
      "|rt @votetimscott ...|[rt, votetimscott...|[official @go, hi...|[rt, votetimscott...|\n",
      "|rt @housegop http...|[rt, housegop, ht...|[@housegop httpst...|[rt, housegop, ht...|\n",
      "|rt @housegop this...|[rt, housegop, jo...|                  []|[rt, housegop, jo...|\n",
      "|rt @potus when i ...|[rt, potus, take,...|[americans have m...|[rt, potus, take,...|\n",
      "|rt @potus heres w...|[rt, potus, stand...|[new jobs, new pr...|[rt, potus, stand...|\n",
      "|rt @potus my admi...|[rt, potus, admin...|[covid19 in their...|[rt, potus, admin...|\n",
      "|@joebiden and wea...|[joebiden, wear, ...|[@joebiden and we...|[joebiden, wear, ...|\n",
      "|@randpaul httpstc...|[randpaul, httpst...|[@randpaul httpst...|[randpaul, httpst...|\n",
      "|@randpaul also ma...|[randpaul, also, ...|[hard as possible...|[randpaul, also, ...|\n",
      "|rt @senwarren the...|[rt, senwarren, l...|[longer it takes,...|[rt, senwarren, l...|\n",
      "|@amyklobuchar ser...|[amyklobuchar, se...|[jab youre, youre...|[amyklobuchar, se...|\n",
      "|@vp i am old enou...|[vp, old, enough,...|[time u, way to t...|[vp, old, enough,...|\n",
      "|rt @vp there is a...|[rt, vp, lot, pan...|[pandemic that is...|[rt, vp, lot, pan...|\n",
      "|rt @potus folks i...|[rt, potus, folk,...|[crumbling roads,...|[rt, potus, folk,...|\n",
      "|rt @joebiden as w...|[rt, joebiden, co...|[new year, best w...|[rt, joebiden, co...|\n",
      "|rt @joebiden the ...|[rt, joebiden, va...|[valuable tool, l...|[rt, joebiden, va...|\n",
      "|@potus just becau...|[potus, people, g...|[work after the l...|[potus, people, g...|\n",
      "|rt @potus heres w...|[rt, potus, stand...|[new jobs, new pr...|[rt, potus, stand...|\n",
      "|rt @senschumer wi...|[rt, senschumer, ...|[judicial nominee...|[rt, senschumer, ...|\n",
      "|rt @potus my admi...|[rt, potus, admin...|[covid19 in their...|[rt, potus, admin...|\n",
      "+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "documentAssembler = DocumentAssembler().setInputCol(\"text\").setOutputCol('document')\n",
    "tokenizer = Tokenizer().setInputCols(['document']).setOutputCol('tokenized')\n",
    "normalizer = Normalizer().setInputCols(['tokenized']).setOutputCol('normalized')\n",
    "lemmatizer = LemmatizerModel.pretrained().setInputCols(['normalized']).setOutputCol('lemmatized')\n",
    "stopwords_cleaner = StopWordsCleaner().setInputCols(['lemmatized'])\\\n",
    ".setOutputCol('unigrams').setStopWords(stopwords.words('english'))\n",
    "pos_tagger = PerceptronModel.pretrained('pos_anc').setInputCols(['document', 'unigrams']).setOutputCol('pos')\n",
    "chunker = Chunker().setInputCols(['document', 'pos']).setOutputCol('ngrams').setRegexParsers(['<JJ>+<NN>', '<NN>+<NN>'])\n",
    "finisher = Finisher().setInputCols(['unigrams', 'ngrams'])\n",
    "pipeline = Pipeline() \\\n",
    "     .setStages([documentAssembler,\n",
    "                 tokenizer,\n",
    "                 normalizer,\n",
    "                 lemmatizer,\n",
    "                 stopwords_cleaner,\n",
    "                 pos_tagger,\n",
    "                 chunker,\n",
    "                 finisher])\n",
    "processed_df = pipeline.fit(df).transform(df).withColumn(\"final\", f.concat(\"finished_unigrams\", \"finished_ngrams\"))\n",
    "processed_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "24050e7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['potus', 'joebiden', 'vp', 'get', 'rt', 'people', 'make', 'go', 'dont', 'covid', 'economy', 'create', 'need', 'say', 'know'] \n",
      "\n",
      "['new', 'rt', 'year', 'potus', 'one', 'president', 'job', 'million', 'stand', 'record', 'nearly', 'new jobs', 'unemployment', 'new president', 'jobs  a record'] \n",
      "\n",
      "['senwarren', 'american', 'rt', 'want', 'law', 'court', 'supreme', 'ready', 'radical', 'supreme court', 'remain', 'land', 'ove', 'roevwade', 'ready to pitch'] \n",
      "\n",
      "['rt', 'randpaul', 'tedcruz', 'vaccine', 'vote', 'care', 'senronjohnson', 'would', 'covid', 'mandate', 'solution', 'right', 'amyklobuchar', 'health', 'treatment'] \n",
      "\n",
      "['rt', 'help', 'pandemic', 'something', 'use', 'end', 'vp', 'power', 'lot', 'last', 'individual', 'federal', 'frustrate', 'every', 'individual power'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "cv = CountVectorizer(inputCol=\"final\", outputCol=\"features\", vocabSize=500, minDF=3.0)\n",
    "cv_model = cv.fit(processed_df)\n",
    "lda = LDA(k=5, maxIter=100)\n",
    "model = lda.fit(cv_model.transform(processed_df))\n",
    "for indices in model.describeTopics(15).select(\"termIndices\").rdd.flatMap(lambda x: x).collect():\n",
    "    print([cv_model.vocabulary[i] for i in indices], '\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
