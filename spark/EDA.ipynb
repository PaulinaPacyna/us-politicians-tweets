{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c103e12",
   "metadata": {},
   "source": [
    "# 0. Libraries and spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95a9b339",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spark-nlp==3.3.4 in /usr/local/lib/python3.6/dist-packages\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install spark-nlp==3.3.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c32acb2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.6/dist-packages (from nltk)\n",
      "Requirement already satisfied: click in /usr/lib/python3/dist-packages (from nltk)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from nltk)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from nltk)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.7.3->pandas)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install numpy pandas nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da483f4c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://ip-172-31-7-229.ec2.internal:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f45453d2c18>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b5ac11a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sparknlp.annotator import LemmatizerModel\n",
    "import pyspark.sql.functions as f\n",
    "from pyspark.sql import Window\n",
    "import pyspark.sql.types as t\n",
    "from pyspark.ml.feature import Tokenizer as pysparkTokenizer, HashingTF, StopWordsRemover, CountVectorizer\n",
    "from pyspark.ml import Pipeline\n",
    "from sparknlp.base import DocumentAssembler, Finisher\n",
    "from sparknlp.annotator import Tokenizer, Normalizer, LemmatizerModel, StopWordsCleaner, PerceptronModel, Chunker\n",
    "from pyspark.ml.clustering import LDA\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c7fe3c",
   "metadata": {},
   "source": [
    "# 1. Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d109779f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|                text|\n",
      "+--------------------+\n",
      "|RT @votetimscott:...|\n",
      "|@RandPaul https:/...|\n",
      "|@RandPaul Also, m...|\n",
      "|RT @VP: There is ...|\n",
      "|RT @POTUS: Folks,...|\n",
      "|RT @JoeBiden: The...|\n",
      "|RT @POTUS: Here’s...|\n",
      "|RT @SenSchumer: W...|\n",
      "|@JoeBiden Oh pleA...|\n",
      "|RT @amyklobuchar:...|\n",
      "|      @JoeBiden Nope|\n",
      "|@SenWarren 70% ? ...|\n",
      "|              @POTUS|\n",
      "|@POTUS Now do gas...|\n",
      "|RT @RepDavidKusto...|\n",
      "|@POTUS We'd have ...|\n",
      "|@RandPaul The law...|\n",
      "|RT @POTUS: Here’s...|\n",
      "|@POTUS I think yo...|\n",
      "|@POTUS I think yo...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Lenght: 3727'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.read.option(\"tableName\", \"Tweets\").format(\"dynamodb\").load().select(f.col(\"text\"))\\\n",
    ".union(\n",
    "    spark.read.option(\"tableName\", \"RedditPosts\").format(\"dynamodb\").load().select(f.col(\"submission_id\").alias(\"text\"))\n",
    ")\n",
    "df.show()\n",
    "f\"Lenght: {df.count()}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5308ca20",
   "metadata": {},
   "source": [
    "Removing special chacaters and changing to lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7c05c398",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|                text|\n",
      "+--------------------+\n",
      "|rt @votetimscott ...|\n",
      "|@randpaul httpstc...|\n",
      "|@randpaul also ma...|\n",
      "|rt @vp there is a...|\n",
      "|rt @potus folks i...|\n",
      "|rt @joebiden the ...|\n",
      "|rt @potus heres w...|\n",
      "|rt @senschumer wi...|\n",
      "|@joebiden oh plea...|\n",
      "|rt @amyklobuchar ...|\n",
      "|      @joebiden nope|\n",
      "|@senwarren 70    ...|\n",
      "|              @potus|\n",
      "|@potus now do gas...|\n",
      "|rt @repdavidkusto...|\n",
      "|@potus wed have m...|\n",
      "|@randpaul the law...|\n",
      "|rt @potus heres w...|\n",
      "|@potus i think yo...|\n",
      "|@potus i think yo...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.withColumn(\"text\", f.lower(f.regexp_replace(f.col(\"text\"), \"[^A-Za-z0-9@ ]\", \"\")))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13867c2a",
   "metadata": {},
   "source": [
    "# 2. LDA - topics analysis - Pyspark only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "76ece113",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+\n",
      "|                text|              tokens|       no stop words|\n",
      "+--------------------+--------------------+--------------------+\n",
      "|rt @votetimscott ...|[rt, @votetimscot...|[rt, @votetimscot...|\n",
      "|@randpaul httpstc...|[@randpaul, https...|[@randpaul, https...|\n",
      "|@randpaul also ma...|[@randpaul, also,...|[@randpaul, also,...|\n",
      "|rt @vp there is a...|[rt, @vp, there, ...|[rt, @vp, lot, pa...|\n",
      "|rt @potus folks i...|[rt, @potus, folk...|[rt, @potus, folk...|\n",
      "+--------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'text': 'rt @votetimscott one of the highlights of my year was giving the official @gop response to joe bidens first address to congress i said i',\n",
       "  'tokens': ['rt',\n",
       "   '@votetimscott',\n",
       "   'one',\n",
       "   'of',\n",
       "   'the',\n",
       "   'highlights',\n",
       "   'of',\n",
       "   'my',\n",
       "   'year',\n",
       "   'was',\n",
       "   'giving',\n",
       "   'the',\n",
       "   'official',\n",
       "   '@gop',\n",
       "   'response',\n",
       "   'to',\n",
       "   'joe',\n",
       "   'bidens',\n",
       "   'first',\n",
       "   'address',\n",
       "   'to',\n",
       "   'congress',\n",
       "   'i',\n",
       "   'said',\n",
       "   'i'],\n",
       "  'no stop words': ['rt',\n",
       "   '@votetimscott',\n",
       "   'one',\n",
       "   'highlights',\n",
       "   'year',\n",
       "   'giving',\n",
       "   'official',\n",
       "   '@gop',\n",
       "   'response',\n",
       "   'joe',\n",
       "   'bidens',\n",
       "   'first',\n",
       "   'address',\n",
       "   'congress',\n",
       "   'said']},\n",
       " {'text': '@randpaul httpstco3leg5o0bp0',\n",
       "  'tokens': ['@randpaul', 'httpstco3leg5o0bp0'],\n",
       "  'no stop words': ['@randpaul', 'httpstco3leg5o0bp0']},\n",
       " {'text': '@randpaul also make voting as hard as possible  reduce poll station number early voting hours and staff at regul httpstcoitmsnlaytr',\n",
       "  'tokens': ['@randpaul',\n",
       "   'also',\n",
       "   'make',\n",
       "   'voting',\n",
       "   'as',\n",
       "   'hard',\n",
       "   'as',\n",
       "   'possible',\n",
       "   '',\n",
       "   'reduce',\n",
       "   'poll',\n",
       "   'station',\n",
       "   'number',\n",
       "   'early',\n",
       "   'voting',\n",
       "   'hours',\n",
       "   'and',\n",
       "   'staff',\n",
       "   'at',\n",
       "   'regul',\n",
       "   'httpstcoitmsnlaytr'],\n",
       "  'no stop words': ['@randpaul',\n",
       "   'also',\n",
       "   'make',\n",
       "   'voting',\n",
       "   'hard',\n",
       "   'possible',\n",
       "   '',\n",
       "   'reduce',\n",
       "   'poll',\n",
       "   'station',\n",
       "   'number',\n",
       "   'early',\n",
       "   'voting',\n",
       "   'hours',\n",
       "   'staff',\n",
       "   'regul',\n",
       "   'httpstcoitmsnlaytr']}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = pysparkTokenizer(inputCol=\"text\", outputCol=\"tokens\")\n",
    "stopwords_cleaner = StopWordsRemover(inputCol=\"tokens\", outputCol=\"no stop words\")\n",
    "nlp_pipeline = Pipeline(\n",
    "    stages=[tokenizer,\n",
    "            stopwords_cleaner])\n",
    "nlp_model = nlp_pipeline.fit(df)\n",
    "processed_df  = nlp_model.transform(df)\n",
    "processed_df.show(5)\n",
    "processed_df.limit(3).toPandas().to_dict(\"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f0f0009a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', 'new', '6', '@potus', 'rt', 'unemployment', 'president', 'jobs', 'one', 'year', 'million', 'stand', 'heres', 'record', 'nearly'] \n",
      "\n",
      "['@potus', 'rt', 'back', 'federal', 'state', 'plan', 'administration', 'every', 'covid19', 'last', 'week', 'people', 'governor', 'fighting', 'covid'] \n",
      "\n",
      "['@randpaul', '@vp', '@joebiden', 'rt', 'vaccine', 'mandates', 'vaccines', 'dont', 'get', 'obvious', 'ulterior', 'motive', 'seem', 'opposing', 'opposition'] \n",
      "\n",
      "['rt', '', '@potus', 'care', 'booster', 'high', 'increase', 'protection', 'covid19', 'shots', 'illness', 'severe', 'degree', 'antibody', 'levels'] \n",
      "\n",
      "['rt', '@joebiden', 'way', 'best', 'year', 'remember', 'protect', 'continue', 'new', 'health', 'head', 'holiday', 'celebrations', 'kee', '@senwarren'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "cv = CountVectorizer(inputCol=\"no stop words\", outputCol=\"features\", vocabSize=500, minDF=3.0)\n",
    "cv_model = cv.fit(processed_df)\n",
    "lda = LDA(k=5, maxIter=100)\n",
    "model = lda.fit(cv_model.transform(processed_df))\n",
    "for indices in model.describeTopics(15).select(\"termIndices\").rdd.flatMap(lambda x: x).collect():\n",
    "    print([cv_model.vocabulary[i] for i in indices], '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00101ca",
   "metadata": {},
   "source": [
    "# 3. LDA - topics analysis - SparkNLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "00280e7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lemma_antbnc download started this may take some time.\n",
      "Approximate size to download 907.6 KB\n",
      "[OK!]\n",
      "pos_anc download started this may take some time.\n",
      "Approximate size to download 3.9 MB\n",
      "[OK!]\n",
      "+--------------------+--------------------+--------------------+--------------------+\n",
      "|                text|   finished_unigrams|     finished_ngrams|               final|\n",
      "+--------------------+--------------------+--------------------+--------------------+\n",
      "|rt @votetimscott ...|[rt, votetimscott...|[official @go, hi...|[rt, votetimscott...|\n",
      "|@randpaul httpstc...|[randpaul, httpst...|[@randpaul httpst...|[randpaul, httpst...|\n",
      "|@randpaul also ma...|[randpaul, also, ...|[hard as possible...|[randpaul, also, ...|\n",
      "|rt @vp there is a...|[rt, vp, lot, pan...|[pandemic that is...|[rt, vp, lot, pan...|\n",
      "|rt @potus folks i...|[rt, potus, folk,...|[crumbling roads,...|[rt, potus, folk,...|\n",
      "|rt @joebiden the ...|[rt, joebiden, va...|[valuable tool, l...|[rt, joebiden, va...|\n",
      "|rt @potus heres w...|[rt, potus, stand...|[new jobs, new pr...|[rt, potus, stand...|\n",
      "|rt @senschumer wi...|[rt, senschumer, ...|[judicial nominee...|[rt, senschumer, ...|\n",
      "|rt @potus my admi...|[rt, potus, admin...|[covid19 in their...|[rt, potus, admin...|\n",
      "|@joebiden oh plea...|[joebiden, oh, pl...|     [cardboard cut]|[joebiden, oh, pl...|\n",
      "|rt @amyklobuchar ...|[rt, amyklobuchar...|[first thing, sta...|[rt, amyklobuchar...|\n",
      "|      @joebiden nope|    [joebiden, nope]|    [@joebiden nope]|[joebiden, nope, ...|\n",
      "|@senwarren 70    ...|   [senwarren, hmmm]|[@senwarren 70   ...|[senwarren, hmmm,...|\n",
      "|              @potus|             [potus]|                  []|             [potus]|\n",
      "|@potus now do gas...|[potus, gas, pric...|[@potus now do ga...|[potus, gas, pric...|\n",
      "|rt @repdavidkusto...|[rt, repdavidkust...|[skyrocketed cost...|[rt, repdavidkust...|\n",
      "|  @joebiden fuck you|    [joebiden, fuck]|    [@joebiden fuck]|[joebiden, fuck, ...|\n",
      "|@potus wed have m...|[potus, wed, resp...|[everything on co...|[potus, wed, resp...|\n",
      "|@randpaul the law...|[randpaul, lawmak...|[corporate invest...|[randpaul, lawmak...|\n",
      "|rt @potus heres w...|[rt, potus, stand...|[new jobs, new pr...|[rt, potus, stand...|\n",
      "+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "documentAssembler = DocumentAssembler().setInputCol(\"text\").setOutputCol('document')\n",
    "tokenizer = Tokenizer().setInputCols(['document']).setOutputCol('tokenized')\n",
    "normalizer = Normalizer().setInputCols(['tokenized']).setOutputCol('normalized')\n",
    "lemmatizer = LemmatizerModel.pretrained().setInputCols(['normalized']).setOutputCol('lemmatized')\n",
    "stopwords_cleaner = StopWordsCleaner().setInputCols(['lemmatized'])\\\n",
    ".setOutputCol('unigrams').setStopWords(stopwords.words('english'))\n",
    "pos_tagger = PerceptronModel.pretrained('pos_anc').setInputCols(['document', 'unigrams']).setOutputCol('pos')\n",
    "chunker = Chunker().setInputCols(['document', 'pos']).setOutputCol('ngrams').setRegexParsers(['<JJ>+<NN>', '<NN>+<NN>'])\n",
    "finisher = Finisher().setInputCols(['unigrams', 'ngrams'])\n",
    "pipeline = Pipeline() \\\n",
    "     .setStages([documentAssembler,\n",
    "                 tokenizer,\n",
    "                 normalizer,\n",
    "                 lemmatizer,\n",
    "                 stopwords_cleaner,\n",
    "                 pos_tagger,\n",
    "                 chunker,\n",
    "                 finisher])\n",
    "processed_df = pipeline.fit(df).transform(df).withColumn(\"final\", f.concat(\"finished_unigrams\", \"finished_ngrams\"))\n",
    "processed_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "24050e7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['joebiden', 'good', 'rt', 'way', 'year', 'continue', 'protect', 'remember', 'new', 'best way', 'head', 'health', 'holiday', 'new year', 'celebration'] \n",
      "\n",
      "['new', 'rt', 'potus', 'president', 'job', 'one', 'year', 'million', 'stand', 'record', 'nearly', 'unemployment', 'new jobs', 'new president', '@potus heres where we stand'] \n",
      "\n",
      "['potus', 'vp', 'get', 'rt', 'tedcruz', 'people', 'joebiden', 'need', 'think', 'go', 'help', 'biden', 'everyone', 'amp', 'dont'] \n",
      "\n",
      "['rt', 'randpaul', 'senwarren', 'vote', 'vaccine', 'want', 'would', 'american', 'court', 'care', 'radical', 'law', 'ready', 'joebiden', 'supreme'] \n",
      "\n",
      "['potus', 'covid', 'rt', 'back', 'federal', 'state', 'plan', 'administration', 'every', 'last', 'week', 'fight', 'governor', 'federal plan', 'see'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "cv = CountVectorizer(inputCol=\"final\", outputCol=\"features\", vocabSize=500, minDF=3.0)\n",
    "cv_model = cv.fit(processed_df)\n",
    "lda = LDA(k=5, maxIter=100)\n",
    "model = lda.fit(cv_model.transform(processed_df))\n",
    "for indices in model.describeTopics(15).select(\"termIndices\").rdd.flatMap(lambda x: x).collect():\n",
    "    print([cv_model.vocabulary[i] for i in indices], '\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
